<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Jannik  ZÃ¼rn | Using a TPU in Google Colab</title>
    <meta name="author" content="Jannik  ZÃ¼rn" />
    <meta name="description" content="Making use of TPU cloud resources" />
    <meta name="keywords" content="jannik zÃ¼rn, computer vision, robotics, freiburg, ai, ml" />


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous" />

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light" />

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸ¤–</text></svg>">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://jzuern.github.io/blog/2018/colab-tpu/">
    
    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark" />

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="https://jzuern.github.io/"><span class="font-weight-bold">Jannik</span>   ZÃ¼rn</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/blog/">blog</a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/projects/">projects</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/teaching/">teaching</a>
              </li>

              <!-- Toogle theme mode -->
              <div class="toggle-container">
                <a id="light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </a>
              </div>
            </ul>
          </div>
        </div>
      </nav>
    </header>

    <!-- Content -->
    <div class="container mt-5">
      <!-- _layouts/post.html -->

<div class="post">

  <header class="post-header">
    <h1 class="post-title">Using a TPU in Google Colab</h1>
    <p class="post-meta">November 7, 2018</p>
    <p class="post-tags">
      <a href="/blog/2018"> <i class="fas fa-calendar fa-sm"></i> 2018 </a>
      Â  Â· Â 
        <a href="/blog/tag/blog">
          <i class="fas fa-hashtag fa-sm"></i> blog</a> Â 
          

    </p>
  </header>

  <article class="post-content">
    <table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="/images/tpu/1.jpg" alt=""></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><em>TPU unit with 4 cores</em></td>
    </tr>
  </tbody>
</table>

<p><a href="https://medium.com/@jannik.zuern/training-a-cifar-10-classifier-in-the-cloud-using-tensorflow-and-google-colab-f3a5fbdfe24d" target="_blank" rel="noopener noreferrer">Last week</a>, we talked about training an image classifier on the CIFAR-10 dataset using Google Colab on a Tesla K80 GPU in the cloud. This time, we will instead carry out the classifier training on a Tensor Processing Unit (TPU).</p>

<blockquote>
  <p>Because training and running deep learning models can be computationally demanding, we built the Tensor Processing Unit (TPU), an ASIC designed from the ground up for machine learning that powers several of our major products, including Translate, Photos, Search, Assistant, and Gmail.</p>
</blockquote>

<p>TPUâ€™s have been recently added to the Google Colab portfolio making it even more attractive for quick-and-dirty machine learning projects when your own local processing units are just not fast enough. While the Tesla K80 available in Google Colab delivers respectable 1.87 TFlops and has 12GB RAM, the <strong>TPUv2</strong> available from within Google Colab comes with a whopping 180 TFlops, give or take. It also comes with 64 GB High Bandwidth Memory (HBM).</p>

<h1 id="enabling-tpu-support-in-the-notebook">Enabling TPU support in the notebook</h1>

<p>In order to try out the TPU on a concrete project, we will work with a Colab notebook, in which a Keras model is trained on classifying the CIFAR-10 dataset. It can be found <a href="https://colab.research.google.com/drive/1ISfhxFDntfOos7cOeT7swduSqzLEqyFn" target="_blank" rel="noopener noreferrer">HERE</a>.</p>

<p>If you would just like to execute the TPU-compatible notebook, you can find it HERE. Otherwise, just follow the next simple steps to add TPU support to an existing notebook.</p>

<p>Enabling TPU support for the notebook is really straightforward. First, letâ€™s change the runtime settings:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="/images/tpu/2.png" alt=""></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Â </td>
    </tr>
  </tbody>
</table>

<p>And choose <strong>TPU</strong> as the hardware accelerator:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="/images/tpu/3.png" alt=""></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Â </td>
    </tr>
  </tbody>
</table>

<h1 id="code-adjustments">Code adjustments</h1>
<p>We also have to make minor adjustments to the Python code in the notebook. We add a new cell anywhere in the notebook in which we check that the TPU devices are properly recognized in the environment:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">pprint</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>

<span class="k">if</span> <span class="s">'COLAB_TPU_ADDR'</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">:</span>
  <span class="k">print</span><span class="p">(</span><span class="s">'ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!'</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
  <span class="n">tpu_address</span> <span class="o">=</span> <span class="s">'grpc://'</span> <span class="o">+</span> <span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="s">'COLAB_TPU_ADDR'</span><span class="p">]</span>
  <span class="k">print</span> <span class="p">(</span><span class="s">'TPU address is'</span><span class="p">,</span> <span class="n">tpu_address</span><span class="p">)</span>

  <span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="n">Session</span><span class="p">(</span><span class="n">tpu_address</span><span class="p">)</span> <span class="k">as</span> <span class="n">session</span><span class="p">:</span>
    <span class="n">devices</span> <span class="o">=</span> <span class="n">session</span><span class="p">.</span><span class="n">list_devices</span><span class="p">()</span>
    
  <span class="k">print</span><span class="p">(</span><span class="s">'TPU devices:'</span><span class="p">)</span>
  <span class="n">pprint</span><span class="p">.</span><span class="n">pprint</span><span class="p">(</span><span class="n">devices</span><span class="p">)</span>
</code></pre></div></div>

<p>This should output a list of 8 TPU devices available in our Colab environment. In order to run the tf.keras model on a TPU, we have to convert it to a TPU-model using the <code class="language-plaintext highlighter-rouge">tf.contrib.tpu.keras_to_tpu</code> module. Luckily, the module takes care of everything for us, leaving us with a couple of lines of boilerplate code.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># This address identifies the TPU we'll use when configuring TensorFlow.
</span><span class="n">TPU_WORKER</span> <span class="o">=</span> <span class="s">'grpc://'</span> <span class="o">+</span> <span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="s">'COLAB_TPU_ADDR'</span><span class="p">]</span>
<span class="n">tf</span><span class="p">.</span><span class="n">logging</span><span class="p">.</span><span class="n">set_verbosity</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">logging</span><span class="p">.</span><span class="n">INFO</span><span class="p">)</span>

<span class="n">resnet_model</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">contrib</span><span class="p">.</span><span class="n">tpu</span><span class="p">.</span><span class="n">keras_to_tpu_model</span><span class="p">(</span>
    <span class="n">resnet_model</span><span class="p">,</span>
    <span class="n">strategy</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">contrib</span><span class="p">.</span><span class="n">tpu</span><span class="p">.</span><span class="n">TPUDistributionStrategy</span><span class="p">(</span>
        <span class="n">tf</span><span class="p">.</span><span class="n">contrib</span><span class="p">.</span><span class="n">cluster_resolver</span><span class="p">.</span><span class="n">TPUClusterResolver</span><span class="p">(</span><span class="n">TPU_WORKER</span><span class="p">)))</span>
</code></pre></div></div>

<p>In case your model is defined using the recently presented <strong>TensorFlow Estimator API</strong>, you only have to make some minor adjustments to your Estimatorâ€™s <code class="language-plaintext highlighter-rouge">model_fn</code> like so:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 
# .... body of model_fn
#
</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">train</span><span class="p">.</span><span class="n">AdamOptimizer</span><span class="p">()</span>
  <span class="k">if</span> <span class="n">FLAGS</span><span class="p">.</span><span class="n">use_tpu</span><span class="p">:</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">contrib</span><span class="p">.</span><span class="n">tpu</span><span class="p">.</span><span class="n">CrossShardOptimizer</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>

  <span class="n">train_op</span> <span class="o">=</span> <span class="n">optimizer</span><span class="p">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">global_step</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">train</span><span class="p">.</span><span class="n">get_global_step</span><span class="p">())</span>
  
<span class="c1">#   return tf.estimator.EstimatorSpec(   # CPU or GPU estimator 
#     mode=mode,
#     loss=loss,
#     train_op=train_op,
#     predictions=predictions)
</span>
  <span class="k">return</span> <span class="n">tf</span><span class="p">.</span><span class="n">contrib</span><span class="p">.</span><span class="n">tpu</span><span class="p">.</span><span class="n">TPUEstimatorSpec</span><span class="p">(</span>  <span class="c1"># TPU estimator 
</span>      <span class="n">mode</span><span class="o">=</span><span class="n">mode</span><span class="p">,</span>
      <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span>
      <span class="n">train_op</span><span class="o">=</span><span class="n">train_op</span><span class="p">,</span>
      <span class="n">predictions</span><span class="o">=</span><span class="n">predictions</span><span class="p">)</span>
</code></pre></div></div>

<p>You can find an example of a <code class="language-plaintext highlighter-rouge">TPUEstimator</code> in the TensorFlow <a href="https://github.com/tensorflow/tpu/blob/master/models/experimental/cifar_keras/cifar_keras.py" target="_blank" rel="noopener noreferrer">GitHub repository</a>.</p>

<p>You should also consider increasing the batch size for training and validation of your model. Since we have 8 TPU units available, a batch size of \(8 \times 128\) might be reasonableâ€Šâ€”â€Šdepending on your modelâ€™s size. Generally speaking, a batch size of \(8 \times 8^n\), \(n\) being \(1, 2, ...\) is advised. Due to the increased batch size, you can experiment with increasing the learning rate as well, making training even faster.</p>

<h1 id="performance-gains">Performance gains</h1>

<p>Compiling the TPU model and initializing the optimizer shards takes time. Depending on the Colab environment workload, it might take a couple of minutes until the first epoch and all the necessary previous initializations have been completed. However, once the TPU model is up and running, it is <em>lightning fast</em>.</p>

<p>Using the Resnet model discussed in the previous post, one epoch takes approximately 25 secs compared to the approx. 7 minutes on the Tesla K80 GPU, resulting in a speedup of almost <strong>17</strong>.</p>

  </article><div id="disqus_thread"></div>
    <script type="text/javascript">
      var disqus_shortname  = 'jzuern-github-io';
      var disqus_identifier = '/blog/2018/colab-tpu';
      var disqus_title      = "Using a TPU in Google Colab";
      (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript" target="_blank" rel="noopener noreferrer">comments powered by Disqus.</a>
</noscript>

</div>

    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        Â© Copyright 2022 Jannik  ZÃ¼rn. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>.

      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.2/dist/umd/popper.min.js" integrity="sha256-l/1pMF/+J4TThfgARS6KwWrk/egwuVvhRzfLAMQ6Ds4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js" integrity="sha256-SyTu6CwrfOhaznYZPoolVw2rxoY7lKYKQvqbtqN93HI=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
  <script src="/assets/js/zoom.js"></script><!-- Load Common JS -->
  <script src="/assets/js/common.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
  </body>
</html>

